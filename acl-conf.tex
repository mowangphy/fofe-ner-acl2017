\documentclass{beamer}
\usetheme{Warsaw}
% \usecolortheme{green}

% \definecolor{PRIMARY}{RGB}{0, 100, 0}
% \definecolor{SECONDARY}{RGB}{144,238,144}
% \setbeamercolor{palette primary}{bg=PRIMARY,fg=white}
% \setbeamercolor{palette secondary}{bg=PRIMARY,fg=white}
% \setbeamercolor{palette tertiary}{bg=PRIMARY,fg=white}
% \setbeamercolor{palette quaternary}{bg=PRIMARY,fg=white}
% \setbeamercolor{structure}{fg=PRIMARY} % itemize, enumerate, etc
% \setbeamercolor{section in toc}{fg=PRIMARY} % TOC sections

% % Override palette coloring with secondary
% \setbeamercolor{subsection in head/foot}{bg=SECONDARY,fg=white}


% not included by beamer
\usepackage{multirow}
\usepackage{bm}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}



\usepackage[style=authoryear]{biblatex}
\bibliography{acl-conf}

\definecolor{darkgreen}{RGB}{0, 100, 0}
\definecolor{navy}{RGB}{0, 0, 128}
\definecolor{darkmagenta}{RGB}{139, 0, 139}
\definecolor{crimson}{RGB}{220, 20, 60}
\definecolor{darkorange}{RGB}{255, 140, 0}

\definecolor{lightyellow}{RGB}{255,255,224}
\definecolor{aliceblue}{RGB}{240,248,255}
\definecolor{grey}{RGB}{192,192,192}



\title[A Local Detection Approach for NER \& MD \hspace{10mm}
		\insertframenumber/\inserttotalframenumber]{
	A Local Detection Approach for \\
	Named Entity Recognition and Mention Detection
}
\author{
	\textbf{Mingbin Xu} \\
	% \vspace{1cm}
	% {\scriptsize Joint work with \\
	% 	\textbf{Hui Jiang} and \textbf{Sedtawut Watcharawittayakul}}
}
\institute{
	Joint work with \textbf{H. Jiang} and \textbf{S. Watcharawittayakul}\\[2ex]

	Lassonade School of Engineering,
	York University,
	Canada

	\begin{figure}
	 	\centering
	  	\includegraphics[width=4.0cm]{logo}\\
	\end{figure}
}
\date{\small ACL2017}
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}


\section{Introduction}

\subsection{Task Definition}

\begin{frame}
\frametitle{Entity Discovery}
\begin{definition}
	A sub-task of information extraction that \textcolor{red}{\textbf{finds}} and \textcolor{red}{\textbf{classifies}} entities in text.
\end{definition}
\begin{example}[CoNLL2003 annotation]
	\textcolor{darkorange}{$[Hinton]_{PER}$}, 
	a professor of \textcolor{navy}{$[University\ of\ Toronto]_{ORG}$}, 
	spends several months in \textcolor{navy}{$[Google]_{ORG}$}'s 
	\textcolor{darkgreen}{$[Mountain\ View]_{LOC}$} office every year.
\end{example}
\begin{columns}
	\column{0.5 \textwidth}
	\column{0.5 \textwidth}
	\begin{description}
		\setlength\itemsep{0.05em}
		\small
		\item[\textcolor{darkorange}{PER}] Person
		\item[\textcolor{darkgreen}{LOC}] Location
		\item[\textcolor{navy}{ORG}] Organization
		\item[\textcolor{darkmagenta}{MISC}] Miscellaneous
	\end{description}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Entity Discovery}
\begin{definition}
	A sub-task of information extraction that \textcolor{red}{\textbf{finds}} and \textcolor{red}{\textbf{classifies}} entities in text.
\end{definition}
\begin{example}[KBP EDL annotation]
	\textcolor{darkorange}{$[Hinton]_{\textit{PER-NAM}}$}, 
	a \textcolor{darkorange}{$[professor]_{\textit{PER-NOM}}$} of \textcolor{navy}{$[University\ of\ \textcolor{darkmagenta}{[Toronto]_{\textit{GPE-NAM}}}]_{\textit{ORG-NAM}}$}, 
	spends several months in \textcolor{navy}{$[Google]_{\textit{ORG-NAM}}$}'s 
	\textcolor{darkgreen}{$[Mountain\ View]_{\textit{LOC-NAM}}$} office every year.
\end{example}
\begin{columns}
	\column{0.3 \textwidth}
	\column{0.7 \textwidth}
	\begin{description}
		\scriptsize
		\setlength\itemsep{0.02em}
		\small
		\item[\textcolor{darkorange}{PER-\{NAME, NOMINAL\}}] Person
		\item[\textcolor{darkgreen}{LOC-\{NAME, NOMINAL\}}] Location
		\item[\textcolor{navy}{ORG-\{NAME, NOMINAL\}}] Organization
		\item[\textcolor{darkmagenta}{GPE-\{NAME, NOMINAL\}}] Geo-Political Entity
		\item[\textcolor{crimson}{FAC-\{NAME, NOMINAL\}}] Facility
	\end{description}
\end{columns}
\end{frame}

\subsection{Review}

\begin{frame}
\frametitle{Review}
\begin{figure}
	\centering
	\resizebox{0.7 \textwidth}{!}{
		\includegraphics{popular-ner}
	}
	\caption{\scriptsize Illustration of popular neuro-NER models}
\end{figure}
\end{frame}



\section{Preliminary}

\subsection{Fixed-size Ordinally Forgetting Encoding}

\begin{frame}
\frametitle{Fixed-size Ordinally Forgetting Encoding}
\begin{definition}[FOFE]
	\begin{itemize}
	\item $S = w_1, w_2, ..., w_n$ is a sequence of any discrete symbols;
	\item $w_i$ is represented as $\bm{e_i}$ in 1-hot representation;
	\item the encoding of a partial sequence up to the $t$-th word is recursively defined as \parencite{zhang2015fixed}:
	\begin{equation}
		\nonumber
		\bm{z_t}=
		\begin{cases}
		\bm{0}, & \text{if}\ t = 0 \\
		\alpha \cdot \bm{z_{t - 1}} + \bm{e_t}, & \text{otherwise}
		\end{cases}  \label{eq-fofe-def}
	\end{equation}
	\item $\alpha \in (0, 1)$ and $t \in \{{\mathbb Z}|1 \le x \le n\}$
	\end{itemize}
\end{definition}
\end{frame}


\begin{frame}
\frametitle{Fixed-size Ordinally Forgetting Encoding}
\begin{alertblock}{}
	Any \textcolor{red}{\textbf{variable length}} sequence is encoded into a \textcolor{red}{\textbf{fixed-size}} vector.
\end{alertblock}
\begin{columns}
	\column{0.28 \textwidth}
	\begin{table}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tabular}{|c|c|}
			\hline
			WORD & 1-HOT \\
			\hline\hline
			$w_0$ & $1000000$ \\
			$w_1$ & $0100000$ \\
			$w_2$ & $0010000$ \\
			$w_3$ & $0001000$ \\
			$w_4$ & $0000100$ \\
			$w_5$ & $0000010$ \\
			$w_6$ & $0000001$ \\
			\hline
		\end{tabular}}
		\caption{\scriptsize Vocab of size 7}
	\end{table}
	\column{0.77 \textwidth}
	\begin{table}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tabular}{|l|l|}
			\hline
			PARTIAL SEQUENCE & FOFE \\
			\hline\hline
			$w_6$ & $0, 0, 0, 0, 0, 0, 1$ \\
			$w_6, w_4$ & $0, 0, 0, 0, 1, 0, \alpha$ \\
			$w_6, w_4, w_5$ & $0, 0, 0, 0, \alpha, 1, \alpha^2$ \\
			$w_6, w_4, w_5, w_0$ & $1, 0, 0, 0, \alpha^2, \alpha, \alpha^3$ \\
			$w_6, w_4, w_5, w_0, w_5$ & $\alpha, 0, 0, 0, \alpha^3, 1 + \alpha^2, \alpha^4$ \\
			$w_6, w_4, w_5, w_0, w_5, w_4$ & $\alpha^2, 0, 0, 0, 1 + \alpha^4, \alpha + \alpha^3, \alpha^5$ \\
			\hline
		\end{tabular}}
		\caption{\scriptsize Partial encoding of $w_6, w_4, w_5, w_0, w_5, w_4$}
	\end{table}
\end{columns}
\end{frame}

\subsection{Uniqueness of FOFE}

\begin{frame}
\frametitle{Uniqueness of FOFE}
\begin{theorem}
	If the forgetting factor $\alpha$ satisfies $0 < \alpha \leq 0.5$, 
	FOFE is unique for any countable vocabulary $V$ and any finite value $T$.
\end{theorem}
\begin{theorem}
	For $0.5 < \alpha < 1 $, given any finite value $T$ and any countable vocabulary $V$,
	FOFE is almost unique everywhere, except only a finite set of countable choices of $\alpha$.
\end{theorem}
% \begin{proof}
% 	% \begin{enumerate}[i]
% 	% \item 
% 	(1) $\sum\limits_{n=0}^{\infty} \alpha^n \le 1$; \\
% 	% \item 
% 	(2) $c_0 \times \alpha^0 + c_1 \times \alpha^1 + ... + c_n \times \alpha^n = 0$, $c_i \in \{-1, 0, 1\}$
% 	% \end{enumerate}
% \end{proof}
\end{frame}


\subsection{Efficiency of FOFE}
\begin{frame}
\frametitle{Computational Efficiency of FOFE}
% \begin{columns}
	% \column{0.51 \textwidth}
	\begin{block}{LSTM}
		One step at a time; each involves 4 matrix multications.
		\begin{align*}
		x & = oneHot([w_1, w_2, ..., w_n]) \times W_{embed}\\
		C_t, h_t & = LSTM(x_t, C_{t-1}, h_{t_1})\\
		enc([w_1, ..., w_n]) & = C_n
		\end{align*}
		% \begin{equation}
		% 	\begin{aligned}
		% 		i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\
		% 		f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\
		% 		f_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\
		% 		{\tilde{C}}_t = \sigma(W_c \cdot [h_{t-1}, x_t] + b_c) \\
		% 		C_t = i_t \times {\tilde{C}}_t + f_t \times C_{t-1} \\
		% 		h_t = o_t \times \sigma(C_t)
		% 	\end{aligned}
		% \end{equation}
	\end{block}
	% \column{0.51 \textwidth}
	\begin{block}{FOFE}
		A single matrix multiplication leads to the final encoding.
		\begin{align*}
			alpha & = [\alpha^{n-1}, \alpha^{n-2}, ... , \alpha, 1] \\
			enc([w_1, ..., w_n])
			& =
				\textbf{\textcolor{red}{(}} alpha
				\times oneHot([w_1, w_2, ..., w_n])
				\textbf{\textcolor{red}{)}} \times W_{embed} \\
			& =
				alpha \times \textbf{\textcolor{red}{(}}
				oneHot([w_1, w_2, ..., w_n]) \times W_{embed}
				\textbf{\textcolor{red}{)}}
		\end{align*}
	\end{block}
% \end{columns}
\end{frame}


% TODO: add general architecture of FOFE4NLP


\section{Local Detection Algorithm}

\subsection{Algorithm}

\begin{frame}
\frametitle{Local Detection}
\begin{block}{Intuition}
	\begin{itemize}
	\item People rarely conduct a global decoding over the entire sentence to pinpoint entities.
	\item The key to accurate local is to have full access to
		\textbf{\textcolor{red}{the fragment itself}}, and
		\textbf{\textcolor{red}{its contextual information}}.
	\item FOFE is a \textbf{\textcolor{red}{lossless}} representation of \textbf{\textcolor{red}{fixed length}}.
	\end{itemize}
\end{block}
\begin{example}
    \begin{itemize}
    \item \textbf{\textcolor{navy}{$[S.E.C.]_{ORG}$}} chief 
            \textbf{\textcolor{darkorange}{$[Mary\ Shapiro]_{PER}$}} left 
            \textbf{\textcolor{darkgreen}{$[Washington]_{LOC}$}} in December. \\
    \item Do the entity types of ``S.E.C'' and ``Washington'' matter? How about:
    \textbf{Our} chief Mary Shapiro left \textbf{us} in December?
    \end{itemize}
\end{example}
\end{frame}
\begin{frame}
\begin{block}{Algorithm}
	\begin{itemize}
	\item Extract features from \textbf{\textcolor{red}{each segment}} and send them to an FFNN.
	\item Remove overlapping / inconsistent labels.
	\end{itemize}
\end{block}
\begin{figure}
	\centering
	\resizebox{0.88 \textwidth}{!}{
		\includegraphics{algo}
	}
	\caption{\scriptsize Illustration of local detection}
\end{figure}
\end{frame}


\subsection{Feature Extraction}

\begin{frame}
\frametitle{Feature Extraction}
\begin{block}{Feature Extraction}
	\footnotesize
	\begin{table}
		%\small
		\centering
		\begin{tabular}{|r|p{0.36\textwidth}|p{0.44\textwidth}|}
		% \resizebox{0.96 \textwidth}{!}{
		% \begin{tabular}{r|l|l}
		\hline
		& text segment & context \\
		\hline \hline
		\multirow{4}{*}{\shortstack{word\\level}}
		& \multirow{2}{*}{\textcolor<2>{red}{BoW}} 
		&   \textcolor<5>{red}{left FOFE excl. text fragment} \\ \cline{3-3}
		& & \textcolor<5>{red}{right FOFE excl. text fragment} \\ \cline{2-3}
		& \multicolumn{2}{c|}{\textcolor<6>{red}{left FOFE incl. text fragment}}\\ \cline{2-3}
		& \multicolumn{2}{c|}{\textcolor<7>{red}{right FOFE incl. text fragment}} \\
		\hline
		\multirow{2}{*}{\shortstack{char\\level}}
		& \textcolor<4>{red}{Char CNN \parencite{kim2015character}} & \multirow{2}{*}{N/A} \\
		& \textcolor<3>{red}{Char FOFE} & \\
		\hline
		\end{tabular}%}
	\end{table}
\end{block}
\begin{example}[\footnotesize examining `Mary Shapiro']
	\footnotesize
	\begin{overprint}
		\centering
		\onslide<1>
			\textbf{\textcolor{navy}{$[S.E.C.]_{ORG}$}} chief
		    \textbf{\textcolor{darkorange}{$[Mary\ Shapiro]_{PER}$}}
		    left \textbf{\textcolor{darkgreen}{$[Washington]_{LOC}$}} in December.
		\onslide<2-4>
			\textbf{\textcolor{navy}{$[S.E.C.]_{ORG}$}} chief
    		\colorbox{grey}{\textbf{\textcolor{darkorange}{$[Mary\ Shapiro]_{PER}$}}}
    		left \textbf{\textcolor{darkgreen}{$[Washington]_{LOC}$}} in December.
		\onslide<5>
			\colorbox{grey}{\textbf{\textcolor{navy}{$[S.E.C.]_{ORG}$}} chief}
		    \textbf{\textcolor{darkorange}{$[Mary\ Shapiro]_{PER}$}}
		    \colorbox{grey}{left \textbf{\textcolor{darkgreen}{$[Washington]_{LOC}$}} in December.}
		\onslide<6>
			\colorbox{grey}{
				\textbf{\textcolor{navy}{$[S.E.C.]_{ORG}$}} chief
		    	\textbf{\textcolor{darkorange}{$[Mary\ Shapiro]_{PER}$}}
		    }
		    left \textbf{\textcolor{darkgreen}{$[Washington]_{LOC}$}} in December.
		\onslide<7>
			\textbf{\textcolor{navy}{$[S.E.C.]_{ORG}$}} chief
			\colorbox{grey}{
		    	\textbf{\textcolor{darkorange}{$[Mary\ Shapiro]_{PER}$}}
		    	left \textbf{\textcolor{darkgreen}{$[Washington]_{LOC}$}} in December.
		    }
	\end{overprint}
\end{example}
\end{frame}

% \subsection{2nd-pass Enhancement}

% \begin{frame}
% \frametitle{2nd-pass Enhancement}
% \begin{block}{2nd-pass Trainer}
% 	If we substitute entities in context with general types, the model
% 	\begin{itemize}
% 	\item captures the semantic roles of other entities,
% 	\item filters out unwanted constructs and sparse combinations, and
% 	\item enables longer context expansion.
% 	\end{itemize}
% \end{block}
% \begin{example}[\scriptsize Examining ``University of Toronto'']
% 	\textcolor{darkorange}{{\textless}PER{\textgreater}}, 
% 	a professor of \textcolor{navy}{$[University\ of\ Toronto]_{ORG}$}, 
% 	spends several months in \textcolor{navy}{{\textless}ORG{\textgreater}}'s 
% 	\textcolor{darkgreen}{{\textless}LOC{\textgreater}} office every year.
% \end{example}
% \end{frame}

% \begin{frame}
% \frametitle{2nd-pass Enhancement}
% \begin{figure}
% 	\centering
% 	\resizebox{0.81 \textwidth}{!}{
% 		\includegraphics{system}
% 	}
% \end{figure}
% \end{frame}


\subsection{Model Advantage}

\begin{frame}
\frametitle{Model Advantages}
% \small
% \begin{block}{Computational Efficiency}
% 	\begin{itemize}
% 		\item FOFE can be expressed as sparse matrix, i.e. $indices$ \& $values$
% 		\item $fofe(seq) \cdot embed = 
% 				[\alpha^{n-1}, ..., \alpha, 1] \cdot lookup(seq, embed)$
% 	\end{itemize}
% \end{block}
\begin{block}{Data Availability}
	Correct annotation for the entire sentence is NOT a must.
	\begin{itemize}
		\item Data annotated with different standard are more usable.
		\item Wikipedia highlights an entity's first appearance.{\scriptsize\parencite{nothman2013learning}}
	\end{itemize}
\end{block}
\begin{alertblock}{Advantage over Known Methods}
	\begin{itemize}
		\item Nested depth control
		\item Feature-engineering-free
	\end{itemize}
\end{alertblock}
\end{frame}



\section{Experiments} 


\subsection{CoNLL2003}

\begin{frame}
\frametitle{CoNLL2003 Shared Task}
\begin{block}{CoNLL2003 Shared Task}
	\begin{itemize}
	\item newswire from the Reuters RCV1 corpus;
	\item tagged with 4 types of \textbf{\textcolor{red}{non-nested}} named entities: 
			\textcolor{darkorange}{Person (PER)}, 
			\textcolor{navy}{Organization (ORG)},
			\textcolor{darkgreen}{Location (LOC)}, and
			\textcolor{darkmagenta}{Miscellaneous (MISC)}.
	\end{itemize}
\end{block}
\begin{table}
    \centering
    \resizebox{0.9 \textwidth}{!}{
    \begin{tabular}{c|ccc|cccc}
         & Articles & Sentences & Tokens & LOC & MISC & ORG & PER \\
        \hline
        train & 946 & 14,987 & 203,621 & 7,140 & 3,438 & 6,321 & 6,600 \\
        dev & 216 & 3,466 & 51,362 & 1,837 & 922 & 1,341 & 1,842  \\
        test & 231 & 3,684 & 46,435 & 1,668 & 702 & 1,661 & 1,617 
    \end{tabular}}
    \caption{\scriptsize Data distribution of CoNLL2003}
    \label{tbl:conll2003stat}
\end{table}
\end{frame}

\begin{frame}
\frametitle{Feature Effectiveness}
\begin{table}
	% \scriptsize
	\centering
	% \resizebox{0.93\linewidth}{!}{
	\resizebox{0.98\linewidth}{!}{
	\begin{tabular}{|l|l|l|lll|}
		\hline
		\multicolumn{3}{|c|}{FEATURE} & P & R & F1\\
		\hline\hline
		\multirow{6}{*}{\shortstack{word\\level}} & 
		\multirow{3}{*}{\shortstack[l]{case-\\insensitive}} &
		context FOFE incl. word fragment & 86.64 & 77.04 & 81.56 \\
		& &context FOFE excl. word fragment & 53.98 & 42.17 & 47.35 \\
		& & BoW of word fragment & 82.92 & 71.85 & 76.99  \\ \cline{2-6} 
		& \multirow{3}{*}{\shortstack[l]{case-\\sensitive}} & 
		context FOFE incl. word fragment & 88.88 & 79.83 &84.12  \\
		& &context FOFE excl. word fragment & 50.91 & 42.46 & 46.30  \\
		& & BoW of word fragment & 85.41 & 74.95 & 79.84  \\ \hline
		\multirow{2}{*}{\shortstack{char\\level}} &
		\multicolumn{2}{l|}{Char FOFE of word fragment} & 67.67 & 52.78 & 59.31  \\
		& \multicolumn{2}{l|}{Char CNN of word fragment} & 78.93 & 69.49 & 73.91 \\ \hline
		\multicolumn{3}{|l|}{all case-insensitive features} &  90.11 & 82.75 &  86.28  \\ 
		\multicolumn{3}{|l|}{all case-sensitive features} & 90.26 & 86.63 & 88.41 \\ 
		\multicolumn{3}{|l|}{all word-level features} & 92.03 & 86.08 & 88.96  \\ \hline
		\multicolumn{3}{|l|}{all word-level \& Char FOFE features} & 91.68 &  88.54 & \bf 90.08 \\
		\multicolumn{3}{|l|}{all word-level \& Char CNN features} & 91.80 & 88.58 & \bf 90.16 \\ \hline
		\multicolumn{3}{|l|}{all word-level \& all char-level features}  & 93.29 &  88.27 &  \bf 90.71  \\
		% \multicolumn{3}{|l|}{all features + {\bf dev set} + 5-fold cross-validation} & 92.58 &  89.31 &  \bf 90.92  \\
		% \multicolumn{3}{|l|}{all features + {\bf 2nd-pass}} & 92.13 &  89.61 &  \bf 90.85  \\
		% \multicolumn{3}{|l|}{all features + {\bf 2nd-pass} + {\bf dev set} + 5-fold cross-validation} & 92.62 &  89.77 &  \bf 91.17 \\
		\hline
	\end{tabular}}
	\caption{\scriptsize Effect of various FOFE feature combinations on the CoNLL2003 test data.}
	\label{tbl:feat-cmp:CoNLL03}
\end{table}
% \pause
%   \vskip-3cm
%   \begin{block}{The Overlay Block}
%     Block Text
%   \end{block}
\end{frame}

\begin{frame}
\frametitle{Comparison between Neural Network Models}
\begin{table}
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{|l|lllll|l|}
		\hline
		 algorithm & word & char & gaz & cap & pos & F1 \\
		\hline\hline
		 {CNN-BLSTM-CRF} \parencite{collobert2011natural} & \cmark & \xmark & \cmark & \cmark & \xmark & 89.59  \\
		 {BLSTM-CRF} \parencite{huang2015bidirectional}  &\cmark & \cmark & \cmark & \cmark & \cmark & 90.10 \\
		 {BLSTM-CRF} \parencite{rondeau2016lstm}  & \cmark & \xmark & \cmark & \cmark & \cmark & 89.28 \\
		{BLSTM-CRF, char-CNN} \parencite{chiu2016named}  & \cmark & \cmark & \cmark & \xmark & \xmark & {\bf 91.62} \\
		{Stack-LSTM-CRF, char-LSTM} \parencite{lample2016neural}  & \cmark & \cmark & \xmark & \xmark & \xmark & {\bf 90.94} \\
		\hline \hline
		{\bf FOFE-NER} (single) & \cmark & \cmark & \xmark & \xmark & \xmark & {\bf 90.71} \\
		% {\bf FOFE-NER} (single) + 2nd-pass  & \cmark & \cmark & \xmark & \xmark & \xmark & {\bf 90.85} \\
		{\bf FOFE-NER} (ensemble) + dev  & \cmark & \cmark & \xmark & \xmark & \xmark & {\bf 90.92} \\
		% {\bf FOFE-NER} (ensemble) + dev + 2nd-pass  & \cmark & \cmark & \xmark & \xmark & \xmark & {\bf 91.17} \\
		\hline
	\end{tabular}}
	\caption{\scriptsize Performance ($F_1$ score) comparison among various neural models reported on the CoNLL dataset, and the different features used in these methods.}
	\label{tbl:nn-cmp:CoNLL03}
\end{table}
\end{frame}




\subsection{EDL in KBP2015}
\label{subsec:kbp2015}

\begin{frame}
\frametitle{EDL in KBP2015}
\begin{block}{EDL Track in KBP2015 \parencite{kbpoverview2015}}
\begin{itemize}
	\item Requires to identify entities \textbf{\textcolor{red}{(including nested entities)}} from English, Chinese and Spanish documents.
	\item 5 entity types are defined, i.e. Person (PER), Geo-political Entity (GPE), Organization (ORG), Location (LOC) and Facility (FAC).
	\item Documents are related but \textbf{\textcolor{red}{non-parallel}} across languages. 
\end{itemize}
\end{block}
\begin{table}
	%\resizebox{\linewidth}{!}{
		\centering
		\begin{tabular}{|l|c|c|c|c|}
			\hline
			& English & Chinese & Spanish & ALL \\
			\hline
			Train & 168 & 147 & 129 & 444 \\
			Eval & 167 & 167 & 166 & 500 \\
			\hline
		\end{tabular}%}
	\caption{\scriptsize Number of Documents in KBP2015}
	\label{tbl:kbp2015stat}
\end{table}
\end{frame}

\begin{frame}
\frametitle{EDL in KBP2015}
\begin{table}
	\resizebox{\linewidth}{!}{
	\centering
	\begin{tabular}{|l|lll|lll|}
		\hline
		\ & \multicolumn{3}{c|}{2015 track best} & \multicolumn{3}{c|}{FOFE-NER} \\
		\cline{2-7}
		\ & $P$ & $R$ & $F_{1}$ & $P$ & $R$ & $F_{1}$ \\
		\hline \hline
		Trilingual & 75.9 & 69.3 & 72.4 & 78.3 & 69.9 & \bf 73.9 \\
		English & 79.2 & 66.7 & \bf 72.4 & 77.1 & 67.8 & 72.2 \\
		Chinese & 79.2 & 74.8 & \bf 76.9 & 79.3 & 71.7 & 75.3 \\
		Spanish & 78.4 & 72.2 & 75.2 & 79.9 & 71.8 & \bf 75.6 \\
		\hline
	\end{tabular}}
	\caption{\scriptsize Entity Discovery Performance of our method on the KBP2015 EDL evaluation data, with comparison to the best systems in KBP2015 official evaluation.}
	\label{tbl:kbp2015cmp}
\end{table}
\end{frame}


\subsection{EDL in KBP2016}

\begin{frame}
\frametitle{EDL in KBP2016}
\begin{block}{EDL Track in KBP2016 \parencite{kbpoverview2016}}
\begin{itemize}
	\item The task is extended to detect \textbf{nominal mentions} of all 5 entity typess. 
	\item We treat nominal mention types as some extra entity types and detect them along with named entities.  
\end{itemize}
\end{block}
\begin{example}
	\textcolor{darkorange}{$[Hinton]_{\textit{PER-NAM}}$}, 
	a \colorbox{lightyellow}{\textcolor{darkorange}{$[professor]_{\textit{PER-NOM}}$}} of \textcolor{navy}{$[University\ of\ \textcolor{darkmagenta}{[Toronto]_{\textit{GPE-NAM}}}]_{\textit{ORG-NAM}}$}, 
	spends several months in \textcolor{navy}{$[Google]_{\textit{ORG-NAM}}$}'s 
	\textcolor{darkgreen}{$[Mountain\ View]_{\textit{LOC-NAM}}$} office every year.
\end{example}
\end{frame}

\begin{frame}
\frametitle{Training Data for KBP2016}
\begin{block}{Training and evaluation data in KBP2015}
	(described before), nominal mentions are not labeled.
\end{block}
\begin{block}{Machine-labeled Wikipedia (WIKI)}
	When terms or names are first mentioned in a Wikipedia article they are often linked to the corresponding Wikipedia page by hyperlinks. 
\end{block} 
\begin{block}{In-house dataset}
	A set of 10,000 English and Chinese documents is manually labeled using some annotation rules similar to the KBP 2016 guidelines.
\end{block}
\end{frame}


\begin{frame}
\frametitle{Dataset Effectivness}
\begin{table}
	\centering
	\begin{tabular}{|l|ll|l|}
		\hline
		training data & P  & R & $F_1$ \\ \hline \hline
		KBP2015 & 0.836 & 0.598 & 0.697 \\
		KBP2015 + WIKI &  0.837 & 0.628 & \bf 0.718 \\
		KBP2015 + in-house & 0.836 & 0.680 & \bf 0.750 \\ 
		\hline
	\end{tabular}
	\caption{\scriptsize Our entity discovery official performance (English only) in KBP2016 is shown as a comparison of three models trained by different combinations of training data sets. }
	\label{tbl:kbp2016dataset}	
\end{table}
\begin{alertblock}{}
	\small
	\textsc{Fofe-Ner} ranks \textcolor{red}{\textbf{2nd}} place in \textcolor{red}{\textbf{first participation}}.
\end{alertblock}
\begin{alertblock}{}
	\small
	\textsc{Fofe-Ner} is the \textcolor{red}{\textbf{best single model}} among all participants.
\end{alertblock}
\end{frame}



% \begin{frame}
% \frametitle{Official Evaluation}
% \begin{table}
% 	\centering
% 	\resizebox{\textwidth}{!}{
% 	\begin{tabular}{|l|lll|lll|lll|}
% 		\hline
% 		\multirow{2}{*}{LANG}  &
% 		\multicolumn{3}{c|}{single} & \multicolumn{3}{c|}{2016 Best}\\
% 		& P & R & F1 & P & R & F1\\
% 		\hline \hline
% 		ENG & 0.836 & 0.680 & 0.750 & 0.846 & 0.710 & 0.772 \\
% 		CMN & 0.789 & 0.625 & 0.698 & 0.789 & 0.737 & 0.762 \\
% 		SPA & 0.835 & 0.602 & 0.700 & 0.839 & 0.656 & 0.736 \\
% 		ALL & 0.819 & 0.639 & {\bf 0.718} & 0.802 & 0.704 & 0.756  \\
% 		\hline 
% 	\end{tabular}}
% 	\caption{\scriptsize Official entity discovery performance of our methods on KBP2016 trilingual EDL track. Nominal mentions in Spanish are ignored since no training data is found for them.}
% 	\label{tbl:kbp2016cmp}
% \end{table}
% \end{frame}



\section{Conclusion}
\begin{frame}
\frametitle{Conclusion}
% \begin{itemize}
% 	\item A local detection approach to NER and MD by applying FFNN on top of FOFE
% 	\item No feature engineering and No external knowledge
% 	\item On a par with state-of-the-art ED system
% \end{itemize}
\begin{block}{}
	A local detection approach to NER and MD by applying FFNN on top of FOFE
\end{block}
\begin{block}{}
	No feature engineering and No external knowledge
\end{block}
\begin{block}{}
	On a par with state-of-the-art ED system
\end{block}
\end{frame}


\appendix

\begin{frame}
\begin{center}
{\huge \textsc{Thank You!}}\\
\ \\
{\huge \textsc{(Q\&A)}}
\ \\
\ \\
\ \\
\begin{description}
	\small
	\item[code] \url{https://github.com/xmb-cipher/fofe-ner}
	\item[demo] \url{http://www.eecs.yorku.ca/~nana/ner-home.html}
\end{description}
\end{center}
\end{frame}

\begin{frame}[noframenumbering,allowframebreaks]
\printbibliography[heading=none]
\end{frame}




\end{document}